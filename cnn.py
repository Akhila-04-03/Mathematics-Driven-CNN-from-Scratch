# -*- coding: utf-8 -*-
"""cnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ubl8oH3KV0bc7ucqsD7ZJveS7vvZztZa
"""

import numpy as np

def conv2d(x, kernel, stride=1, padding=0):

    batch_size, h, w, c = x.shape
    kernel_size = kernel.shape[0]
    pad_x = np.pad(x, ((0,), (padding,), (padding,), (0,)), mode='constant')  # Pad the input

    # Calculate the output dimensions after convolution
    output_height = (h + 2 * padding - kernel_size) // stride + 1
    output_width = (w + 2 * padding - kernel_size) // stride + 1
    output = np.zeros((batch_size, output_height, output_width, kernel.shape[3]))  # Output dimensions

    for i in range(output_height):
        for j in range(output_width):
            x_slice = pad_x[:, i*stride:i*stride+kernel_size, j*stride:j*stride+kernel_size, :]
            output[:, i, j, :] = np.tensordot(x_slice, kernel, axes=((1, 2, 3), (0, 1, 2)))  # Convolution operation
    return output


def max_pool2d(x, pool_size=2, stride=2):
    batch_size, h, w, c = x.shape
    pool_height, pool_width = pool_size, pool_size
    output_height = (h - pool_height) // stride + 1  # Output dimensions after pooling
    output_width = (w - pool_width) // stride + 1
    output = np.zeros((batch_size, output_height, output_width, c))  # Output after pooling

    for i in range(output_height):
        for j in range(output_width):
            x_slice = x[:, i*stride:i*stride+pool_height, j*stride:j*stride+pool_width, :]
            output[:, i, j, :] = np.max(x_slice, axis=(1, 2))  # Max pooling operation
    return output


def relu(x):

    return np.maximum(0, x)


def batch_norm(x, gamma=1, beta=0, epsilon=1e-5):
    mean = np.mean(x, axis=(0, 1, 2), keepdims=True)  # Mean across the batch and spatial dimensions
    var = np.var(x, axis=(0, 1, 2), keepdims=True)  # Variance across the batch and spatial dimensions
    x_norm = (x - mean) / np.sqrt(var + epsilon)  # Normalizing the input
    return gamma * x_norm + beta  # Scaling and shifting with gamma and beta


def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # Subtract max for numerical stability
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)  # Normalize to get probabilities


def dense(x, weights, bias):
    return np.dot(x, weights) + bias


def cnn(x, conv_kernel, conv_bias, pool_size=2, fc_weights=None, fc_bias=None):
    # Convolution Layer
    conv_out = conv2d(x, conv_kernel)  # Apply convolution

    # Batch Normalization
    bn_out = batch_norm(conv_out)  # Apply batch normalization

    # ReLU Activation
    relu_out = relu(bn_out)  # Apply ReLU activation

    # Max Pooling
    pool_out = max_pool2d(relu_out, pool_size)  # Apply max pooling

    # Flatten the output from 4dim to 2dim (batch_size, -1)
    flattened = pool_out.flatten().reshape(pool_out.shape[0], -1)  # Flatten to a vector

    # Calculate the flattened size dynamically
    flattened_size = np.prod(flattened.shape[1:])

    # Initialize fully connected weights and biases if not provided
    if fc_weights is None:
        fc_weights = np.random.randn(flattened_size, num_classes)  # Shape: (flattened_size, num_classes)
    if fc_bias is None:
        fc_bias = np.random.randn(num_classes)  # Bias for each class

    # Fully Connected Layer (Dense)
    fc_out = dense(flattened, fc_weights, fc_bias)
    # Output Layer with Softmax
    output = softmax(fc_out)  # Apply softmax to get final probabilities

    return output

batch_size = 32
image_size = 28
num_channels = 1
num_classes = 10

x_input = x_train_augmented[:batch_size]  # Taking first batch of 32 images from x_train

# Convolutional kernel: 3x3 filters, 1 input channel, 10 output channels
conv_kernel = np.random.randn(3, 3, num_channels, 10)

# Convolutional bias (one for each output channel)
conv_bias = np.random.randn(10)

# Fully connected weights and biases (for output layer)
# Calculate the size of the fully connected layer input based on the flattened output after pooling
pool_out_example = max_pool2d(relu(batch_norm(conv2d(x_input, conv_kernel))), pool_size=2)
flattened_size = np.prod(pool_out_example.shape[1:])  #  compute flattened size

# Adjust the weights matrix accordingly
fc_weights = np.random.randn(flattened_size, num_classes)
fc_bias = np.random.randn(num_classes)

# Get the final output from the CNN
output = cnn(x_input, conv_kernel, conv_bias, fc_weights=fc_weights, fc_bias=fc_bias)
print("Output shape:", output.shape)  # (batch_size, num_classes)
predicted_classes = np.argmax(output, axis=1)